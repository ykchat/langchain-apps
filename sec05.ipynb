{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LangChain å…¥é–€ï¼ˆå¾ŒåŠï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "So far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\". For more concrete ideas on the latter, see this awesome paper.\n",
      "\n",
      "LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory.\n",
      "\n",
      "â›“ï¸ IntroduÃ§Ã£o ao Langchain - #Cortes - Live DataHackers by Prof. JoÃ£o Gabriel Lima\n",
      "\n",
      "â›“ï¸ LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity\n",
      "\n",
      "â›“ï¸ KI schreibt krasses Youtube Skript ğŸ˜²ğŸ˜³ | LangChain Tutorial Deutsch by SimpleKI\n",
      "\n",
      "â›“ï¸ Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI by AI Anytime\n",
      "\n",
      "â›“ï¸ QA over documents with Auto vector index selection with Langchain router chains by echohive\n",
      "\n",
      "â›“ï¸ Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox\n",
      "\n",
      "â›“ï¸ Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude! by Chris Alexiuk\n",
      "\n",
      "â›“ï¸ LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra\n",
      "\n",
      "â›“ï¸ LANGCHAIN AI AUTONOMOUS AGENT WEB APP - ğŸ‘¶ BABY AGI ğŸ¤– with EMAIL AUTOMATION using DATABUTTON by Avra\n",
      "\n",
      "â›“ï¸ The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data\n",
      "\n",
      "5 Levels Of LLM Summarizing: Novice to Expert\n",
      "\n",
      "LangChain How to and guides by\n",
      "\n",
      "Sam Witteveen:\n",
      "-\n",
      "\n",
      "LangChain Basics - LLMs & PromptTemplates with Colab\n",
      "-\n",
      "\n",
      "LangChain Basics - Tools and Chains\n",
      "-\n",
      "\n",
      "ChatGPT API Announcement & Code Walkthrough with LangChain\n",
      "-\n",
      "\n",
      "Conversations with Memory (explanation & code walkthrough)\n",
      "-\n",
      "\n",
      "Chat with Flan20B\n",
      "-\n",
      "\n",
      "Using Hugging Face Models locally (code walkthrough)\n",
      "-\n",
      "\n",
      "PAL : Program-aided Language Models with LangChain code\n",
      "-\n",
      "\n",
      "Building a Summarization System with LangChain and GPT-3 - Part 1\n",
      "-\n",
      "\n",
      "Building a Summarization System with LangChain and GPT-3 - Part 2\n",
      "-\n",
      "\n",
      "Microsoft's Visual ChatGPT using LangChain\n",
      "-\n",
      "\n",
      "LangChain Agents - Joining Tools and Chains with Decisions\n",
      "-\n",
      "\n",
      "Comparing LLMs with LangChain\n",
      "-\n",
      "\n",
      "Using Constitutional AI in LangChain\n",
      "-\n",
      "\n",
      "Talking to Alpaca with LangChain - Creating an Alpaca Chatbot\n",
      "-\n",
      "\n",
      "Talk to your CSV & Excel with LangChain\n",
      "-\n",
      "\n",
      "BabyAGI: Discover the Power of Task-Driven Autonomous Agents!\n",
      "-\n",
      "\n",
      "Improve your BabyAGI with LangChain\n",
      "- â›“\n",
      "\n",
      "Interacting with APIs\n",
      "\n",
      "Conceptual Guide\n",
      "\n",
      "Lots of data and information is stored behind APIs.\n",
      "This page covers all resources available in LangChain for working with APIs.\n",
      "\n",
      "Chains\n",
      "\n",
      "If you are just getting started, and you have relatively simple apis, you should get started with chains.\n",
      "Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you \n",
      "understand what is happening better.\n",
      "\n",
      "API Chain\n",
      "\n",
      "Agents\n",
      "\n",
      "Agents are more complex, and involve multiple queries to the LLM to understand what to do.\n",
      "The downside of agents are that you have less control. The upside is that they are more powerful,\n",
      "which allows you to use them on larger and more complex schemas.\n",
      "\n",
      "OpenAPI Agent\n",
      "Human: LangChainã®æ¦‚è¦ã‚’1æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "LangChainã¯ã€APIã¨ã®é€£æºã‚„ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ä½œæˆãªã©ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã¨ãƒã‚§ãƒ¼ãƒ³ã‚’æä¾›ã™ã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "result = index.query(\"LangChainã®æ¦‚è¦ã‚’1æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\", llm=chat)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã¯ã„ã€ã‚ãªãŸã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = ConversationChain(llm=chat)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI: ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "Human: ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the \"pwd\" command to find out the current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m/home/kakutani/code/openai/udemy/langchain-apps\n",
      "\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:I can use the \"pwd\" command to find out the current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\n",
      "Observation: /home/kakutani/code/openai/udemy/langchain-apps\n",
      "\n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kakutani/code/openai/udemy/langchain-apps/env/lib/python3.11/site-packages/langchain/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The current directory is /home/kakutani/code/openai/udemy/langchain-apps\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The current directory is /home/kakutani/code/openai/udemy/langchain-apps\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "tools = load_tools([\"terminal\"], llm=chat)\n",
    "agent_chain = initialize_agent(tools, chat, agent=\"zero-shot-react-description\")\n",
    "\n",
    "result = agent_chain.run(\"What is your current directory?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8. Chat API ã«ç‰¹åŒ–ã—ãŸå®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=876 request_id=d875b8459e54b5640996684058967e28 response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oMKdiQXW3ZFuG4k4QUoseXISMgpl\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692236599,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 74,\\n    \"completion_tokens\": 37,\\n    \"total_tokens\": 111\\n  }\\n}\\n' headers='{\\'Date\\': \\'Thu, 17 Aug 2023 01:43:20 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'876\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89908\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'60ms\\', \\'x-request-id\\': \\'d875b8459e54b5640996684058967e28\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7e39b9d898e08a-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\\\\nAI: \\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u5ca1\\\\u90e8\\\\u3055\\\\u3093\\\\uff01\\\\u304a\\\\u540d\\\\u524d\\\\u3092\\\\u6559\\\\u3048\\\\u3066\\\\u304f\\\\u308c\\\\u3066\\\\u3042\\\\u308a\\\\u304c\\\\u3068\\\\u3046\\\\u3054\\\\u3056\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u3069\\\\u306e\\\\u3088\\\\u3046\\\\u306a\\\\u304a\\\\u624b\\\\u4f1d\\\\u3044\\\\u304c\\\\u3067\\\\u304d\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u308f\\\\u304b\\\\u308a\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI: ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "Human: ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1150 request_id=b230d81005b91c957f25b6a8a9f39dff response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oMKfkLBOYSR1dkk3iB9kUkgX9Qgu\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692236601,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 128,\\n    \"completion_tokens\": 29,\\n    \"total_tokens\": 157\\n  }\\n}\\n' headers='{\\'Date\\': \\'Thu, 17 Aug 2023 01:43:22 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1150\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89861\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'92ms\\', \\'x-request-id\\': \\'b230d81005b91c957f25b6a8a9f39dff\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7e39c2cb52e08a-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u512a\\\\u79c0\\\\u306a\\\\u30a2\\\\u30b7\\\\u30b9\\\\u30bf\\\\u30f3\\\\u30c8\\\\u3067\\\\u3059\\\\u3002\"}, {\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ã¯ã˜ã‚ã¾ã—ã¦ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ' additional_kwargs={} example=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1109 request_id=fbc5cf83a935e7b72265a4ae3ef67b8d response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oMKl0Qo0aqRaDnCXYJsNgeXaia2k\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692236607,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã¯ã˜ã‚ã¾ã—ã¦ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 38,\\n    \"completion_tokens\": 26,\\n    \"total_tokens\": 64\\n  }\\n}\\n' headers='{\\'Date\\': \\'Thu, 17 Aug 2023 01:43:28 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1109\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89961\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'25ms\\', \\'x-request-id\\': \\'fbc5cf83a935e7b72265a4ae3ef67b8d\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7e39e6f909e08a-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\"),\n",
    "    HumanMessage(content=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "]\n",
    "\n",
    "result = chat(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=786 request_id=e358abe128e4b3bb7d0a82b079381d21 response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oMKu03UqPdiSTlod2TiHcjvJWcda\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692236616,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 17,\\n    \"completion_tokens\": 22,\\n    \"total_tokens\": 39\\n  }\\n}\\n' headers='{\\'Date\\': \\'Thu, 17 Aug 2023 01:43:37 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'786\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89974\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'16ms\\', \\'x-request-id\\': \\'e358abe128e4b3bb7d0a82b079381d21\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7e3a22e850e08a-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}, {\"role\": \"assistant\", \"content\": \"\\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u5ca1\\\\u90e8\\\\u3055\\\\u3093\\\\u3002\\\\u3069\\\\u306e\\\\u3088\\\\u3046\\\\u306a\\\\u3054\\\\u7528\\\\u4ef6\\\\u3067\\\\u3057\\\\u3087\\\\u3046\\\\u304b\\\\uff1f\"}, {\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u308f\\\\u304b\\\\u308a\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
      "ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1096 request_id=747c598921b857583014e8479f0e6617 response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oMKvIVeF6k5CaxO7h0fg7U9XisRz\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692236617,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 58,\\n    \"completion_tokens\": 29,\\n    \"total_tokens\": 87\\n  }\\n}\\n' headers='{\\'Date\\': \\'Thu, 17 Aug 2023 01:43:38 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1096\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89944\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'36ms\\', \\'x-request-id\\': \\'747c598921b857583014e8479f0e6617\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7e3a298ef2e08a-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.chat_memory.add_user_message(\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "ai_message = chat(memory.chat_memory.messages).content\n",
    "memory.chat_memory.add_ai_message(ai_message)\n",
    "print(ai_message)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "ai_message = chat(memory.chat_memory.messages).content\n",
    "memory.chat_memory.add_ai_message(ai_message)\n",
    "print(ai_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
