{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LangChain 入門（後半）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "So far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\". For more concrete ideas on the latter, see this awesome paper.\n",
      "\n",
      "LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory.\n",
      "\n",
      "⛓️ Introdução ao Langchain - #Cortes - Live DataHackers by Prof. João Gabriel Lima\n",
      "\n",
      "⛓️ LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity\n",
      "\n",
      "⛓️ KI schreibt krasses Youtube Skript 😲😳 | LangChain Tutorial Deutsch by SimpleKI\n",
      "\n",
      "⛓️ Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI by AI Anytime\n",
      "\n",
      "⛓️ QA over documents with Auto vector index selection with Langchain router chains by echohive\n",
      "\n",
      "⛓️ Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox\n",
      "\n",
      "⛓️ Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude! by Chris Alexiuk\n",
      "\n",
      "⛓️ LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra\n",
      "\n",
      "⛓️ LANGCHAIN AI AUTONOMOUS AGENT WEB APP - 👶 BABY AGI 🤖 with EMAIL AUTOMATION using DATABUTTON by Avra\n",
      "\n",
      "⛓️ The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data\n",
      "\n",
      "Code Understanding\n",
      "\n",
      "Overview\n",
      "\n",
      "LangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\n",
      "\n",
      "Conversational Retriever Chain\n",
      "\n",
      "Conversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\n",
      "\n",
      "LangChain Workflow for Code Understanding and Generation\n",
      "\n",
      "5 Levels Of LLM Summarizing: Novice to Expert\n",
      "\n",
      "LangChain How to and guides by\n",
      "\n",
      "Sam Witteveen:\n",
      "-\n",
      "\n",
      "LangChain Basics - LLMs & PromptTemplates with Colab\n",
      "-\n",
      "\n",
      "LangChain Basics - Tools and Chains\n",
      "-\n",
      "\n",
      "ChatGPT API Announcement & Code Walkthrough with LangChain\n",
      "-\n",
      "\n",
      "Conversations with Memory (explanation & code walkthrough)\n",
      "-\n",
      "\n",
      "Chat with Flan20B\n",
      "-\n",
      "\n",
      "Using Hugging Face Models locally (code walkthrough)\n",
      "-\n",
      "\n",
      "PAL : Program-aided Language Models with LangChain code\n",
      "-\n",
      "\n",
      "Building a Summarization System with LangChain and GPT-3 - Part 1\n",
      "-\n",
      "\n",
      "Building a Summarization System with LangChain and GPT-3 - Part 2\n",
      "-\n",
      "\n",
      "Microsoft's Visual ChatGPT using LangChain\n",
      "-\n",
      "\n",
      "LangChain Agents - Joining Tools and Chains with Decisions\n",
      "-\n",
      "\n",
      "Comparing LLMs with LangChain\n",
      "-\n",
      "\n",
      "Using Constitutional AI in LangChain\n",
      "-\n",
      "\n",
      "Talking to Alpaca with LangChain - Creating an Alpaca Chatbot\n",
      "-\n",
      "\n",
      "Talk to your CSV & Excel with LangChain\n",
      "-\n",
      "\n",
      "BabyAGI: Discover the Power of Task-Driven Autonomous Agents!\n",
      "-\n",
      "\n",
      "Improve your BabyAGI with LangChain\n",
      "- ⛓\n",
      "Human: LangChainの概要を1文で説明してください。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "LangChainはGitHubのコードリポジトリを解析し、コードの理解、文脈に基づいた質問応答、コード生成をサポートするツールです。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "result = index.query(\"LangChainの概要を1文で説明してください。\", llm=chat)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 私の名前は岡部です。\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "こんにちは、岡部さん！お名前を教えてくれてありがとうございます。私はAIです。どのようにお手伝いできますか？\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 私の名前はわかりますか？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "はい、あなたの名前はわかりますか？\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = ConversationChain(llm=chat)\n",
    "result = chain.predict(input=\"私の名前は岡部です。\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat)\n",
    "result = chain.predict(input=\"私の名前はわかりますか？\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 私の名前は岡部です。\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "こんにちは、岡部さん！お名前を教えてくれてありがとうございます。どのようなお手伝いができますか？\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 私の名前は岡部です。\n",
      "AI: こんにちは、岡部さん！お名前を教えてくれてありがとうございます。どのようなお手伝いができますか？\n",
      "Human: 私の名前はわかりますか？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "はい、先ほどおっしゃった通り、お名前は岡部さんですね。\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"私の名前は岡部です。\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"私の名前はわかりますか？\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the terminal to check the current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m/home/kakutani/code/openai/udemy/langchain-apps\n",
      "\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:I can use the terminal to check the current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\n",
      "Observation: /home/kakutani/code/openai/udemy/langchain-apps\n",
      "\n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kakutani/code/openai/udemy/langchain-apps/env/lib/python3.10/site-packages/langchain/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: My current directory is /home/kakutani/code/openai/udemy/langchain-apps\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "My current directory is /home/kakutani/code/openai/udemy/langchain-apps\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "tools = load_tools([\"terminal\"], llm=chat)\n",
    "agent_chain = initialize_agent(tools, chat, agent=\"zero-shot-react-description\")\n",
    "\n",
    "result = agent_chain.run(\"What is your current directory?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8. Chat API に特化した実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 私の名前は岡部です。\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1714 request_id=8f303d0d8c45757414d591db76282ad0 response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsKnPDdpoztw8ocKubqMAqTdOhh\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196404,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"こんにちは、岡部さん！お名前を教えていただきありがとうございます。私はAIです。どのようなお手伝いができますか？\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 74,\\n    \"completion_tokens\": 42,\\n    \"total_tokens\": 116\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:25 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1714\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89908\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'60ms\\', \\'x-request-id\\': \\'8f303d0d8c45757414d591db76282ad0\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a64644bb31d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\\\\nAI: \\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u5ca1\\\\u90e8\\\\u3055\\\\u3093\\\\uff01\\\\u304a\\\\u540d\\\\u524d\\\\u3092\\\\u6559\\\\u3048\\\\u3066\\\\u3044\\\\u305f\\\\u3060\\\\u304d\\\\u3042\\\\u308a\\\\u304c\\\\u3068\\\\u3046\\\\u3054\\\\u3056\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u79c1\\\\u306fAI\\\\u3067\\\\u3059\\\\u3002\\\\u3069\\\\u306e\\\\u3088\\\\u3046\\\\u306a\\\\u304a\\\\u624b\\\\u4f1d\\\\u3044\\\\u304c\\\\u3067\\\\u304d\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u308f\\\\u304b\\\\u308a\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "こんにちは、岡部さん！お名前を教えていただきありがとうございます。私はAIです。どのようなお手伝いができますか？\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 私の名前は岡部です。\n",
      "AI: こんにちは、岡部さん！お名前を教えていただきありがとうございます。私はAIです。どのようなお手伝いができますか？\n",
      "Human: 私の名前はわかりますか？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "はい、先ほどおっしゃった通り、あなたの名前は岡部さんですね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1572 request_id=4703e2d9a99c75684305563687e8040f response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsMOYpkILkpUWPHVOxoF849tGil\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196406,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"はい、先ほどおっしゃった通り、あなたの名前は岡部さんですね。\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 133,\\n    \"completion_tokens\": 32,\\n    \"total_tokens\": 165\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:27 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1572\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89856\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'96ms\\', \\'x-request-id\\': \\'4703e2d9a99c75684305563687e8040f\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a6470de8e1d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"私の名前は岡部です。\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"私の名前はわかりますか？\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u512a\\\\u79c0\\\\u306a\\\\u30a2\\\\u30b7\\\\u30b9\\\\u30bf\\\\u30f3\\\\u30c8\\\\u3067\\\\u3059\\\\u3002\"}, {\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='はじめまして、岡部さん。どのようにお手伝いできますか？' additional_kwargs={} example=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=980 request_id=952a55bb09770f496ac1937982d8dd4a response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsNNrk2OB8N7w0SxuzEpJmkGGV6\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196407,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"はじめまして、岡部さん。どのようにお手伝いできますか？\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 38,\\n    \"completion_tokens\": 26,\\n    \"total_tokens\": 64\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:28 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'980\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89961\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'25ms\\', \\'x-request-id\\': \\'952a55bb09770f496ac1937982d8dd4a\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a647c19941d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは優秀なアシスタントです。\"),\n",
    "    HumanMessage(content=\"私の名前は岡部です。\")\n",
    "]\n",
    "\n",
    "result = chat(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1073 request_id=19532ded7e2a5bbd9d8b380de8cacf3c response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsP3ef0gHKUxYVWuXyV6irV2Al6\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196409,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"こんにちは、岡部さん。どのようなご用件でしょうか？\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 17,\\n    \"completion_tokens\": 22,\\n    \"total_tokens\": 39\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:30 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1073\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89974\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'16ms\\', \\'x-request-id\\': \\'19532ded7e2a5bbd9d8b380de8cacf3c\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a648418b81d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}, {\"role\": \"assistant\", \"content\": \"\\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u5ca1\\\\u90e8\\\\u3055\\\\u3093\\\\u3002\\\\u3069\\\\u306e\\\\u3088\\\\u3046\\\\u306a\\\\u3054\\\\u7528\\\\u4ef6\\\\u3067\\\\u3057\\\\u3087\\\\u3046\\\\u304b\\\\uff1f\"}, {\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u308f\\\\u304b\\\\u308a\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは、岡部さん。どのようなご用件でしょうか？\n",
      "はい、先ほどおっしゃった通り、お名前は岡部さんですね。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1338 request_id=862aa701f046395c463e6ab7a5e90787 response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsQFJOcNalIgyffkfGWPUIxpXR4\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196410,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"はい、先ほどおっしゃった通り、お名前は岡部さんですね。\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 58,\\n    \"completion_tokens\": 29,\\n    \"total_tokens\": 87\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:31 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1338\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89944\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'36ms\\', \\'x-request-id\\': \\'862aa701f046395c463e6ab7a5e90787\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a648cb82d1d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.chat_memory.add_user_message(\"私の名前は岡部です。\")\n",
    "ai_message = chat(memory.chat_memory.messages).content\n",
    "memory.chat_memory.add_ai_message(ai_message)\n",
    "print(ai_message)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"私の名前はわかりますか？\")\n",
    "ai_message = chat(memory.chat_memory.messages).content\n",
    "memory.chat_memory.add_ai_message(ai_message)\n",
    "print(ai_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
