{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LangChain å…¥é–€ï¼ˆå¾ŒåŠï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "So far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\". For more concrete ideas on the latter, see this awesome paper.\n",
      "\n",
      "LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory.\n",
      "\n",
      "â›“ï¸ IntroduÃ§Ã£o ao Langchain - #Cortes - Live DataHackers by Prof. JoÃ£o Gabriel Lima\n",
      "\n",
      "â›“ï¸ LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity\n",
      "\n",
      "â›“ï¸ KI schreibt krasses Youtube Skript ğŸ˜²ğŸ˜³ | LangChain Tutorial Deutsch by SimpleKI\n",
      "\n",
      "â›“ï¸ Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI by AI Anytime\n",
      "\n",
      "â›“ï¸ QA over documents with Auto vector index selection with Langchain router chains by echohive\n",
      "\n",
      "â›“ï¸ Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox\n",
      "\n",
      "â›“ï¸ Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude! by Chris Alexiuk\n",
      "\n",
      "â›“ï¸ LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra\n",
      "\n",
      "â›“ï¸ LANGCHAIN AI AUTONOMOUS AGENT WEB APP - ğŸ‘¶ BABY AGI ğŸ¤– with EMAIL AUTOMATION using DATABUTTON by Avra\n",
      "\n",
      "â›“ï¸ The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data\n",
      "\n",
      "Code Understanding\n",
      "\n",
      "Overview\n",
      "\n",
      "LangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.\n",
      "\n",
      "Conversational Retriever Chain\n",
      "\n",
      "Conversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.\n",
      "\n",
      "LangChain Workflow for Code Understanding and Generation\n",
      "\n",
      "5 Levels Of LLM Summarizing: Novice to Expert\n",
      "\n",
      "LangChain How to and guides by\n",
      "\n",
      "Sam Witteveen:\n",
      "-\n",
      "\n",
      "LangChain Basics - LLMs & PromptTemplates with Colab\n",
      "-\n",
      "\n",
      "LangChain Basics - Tools and Chains\n",
      "-\n",
      "\n",
      "ChatGPT API Announcement & Code Walkthrough with LangChain\n",
      "-\n",
      "\n",
      "Conversations with Memory (explanation & code walkthrough)\n",
      "-\n",
      "\n",
      "Chat with Flan20B\n",
      "-\n",
      "\n",
      "Using Hugging Face Models locally (code walkthrough)\n",
      "-\n",
      "\n",
      "PAL : Program-aided Language Models with LangChain code\n",
      "-\n",
      "\n",
      "Building a Summarization System with LangChain and GPT-3 - Part 1\n",
      "-\n",
      "\n",
      "Building a Summarization System with LangChain and GPT-3 - Part 2\n",
      "-\n",
      "\n",
      "Microsoft's Visual ChatGPT using LangChain\n",
      "-\n",
      "\n",
      "LangChain Agents - Joining Tools and Chains with Decisions\n",
      "-\n",
      "\n",
      "Comparing LLMs with LangChain\n",
      "-\n",
      "\n",
      "Using Constitutional AI in LangChain\n",
      "-\n",
      "\n",
      "Talking to Alpaca with LangChain - Creating an Alpaca Chatbot\n",
      "-\n",
      "\n",
      "Talk to your CSV & Excel with LangChain\n",
      "-\n",
      "\n",
      "BabyAGI: Discover the Power of Task-Driven Autonomous Agents!\n",
      "-\n",
      "\n",
      "Improve your BabyAGI with LangChain\n",
      "- â›“\n",
      "Human: LangChainã®æ¦‚è¦ã‚’1æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "LangChainã¯GitHubã®ã‚³ãƒ¼ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’è§£æã—ã€ã‚³ãƒ¼ãƒ‰ã®ç†è§£ã€æ–‡è„ˆã«åŸºã¥ã„ãŸè³ªå•å¿œç­”ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "result = index.query(\"LangChainã®æ¦‚è¦ã‚’1æ–‡ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\", llm=chat)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ç§ã¯AIã§ã™ã€‚ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã¯ã„ã€ã‚ãªãŸã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = ConversationChain(llm=chat)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI: ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "Human: ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the terminal to check the current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m/home/kakutani/code/openai/udemy/langchain-apps\n",
      "\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:I can use the terminal to check the current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\n",
      "Observation: /home/kakutani/code/openai/udemy/langchain-apps\n",
      "\n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kakutani/code/openai/udemy/langchain-apps/env/lib/python3.10/site-packages/langchain/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: My current directory is /home/kakutani/code/openai/udemy/langchain-apps\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "My current directory is /home/kakutani/code/openai/udemy/langchain-apps\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "tools = load_tools([\"terminal\"], llm=chat)\n",
    "agent_chain = initialize_agent(tools, chat, agent=\"zero-shot-react-description\")\n",
    "\n",
    "result = agent_chain.run(\"What is your current directory?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8. Chat API ã«ç‰¹åŒ–ã—ãŸå®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\n\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1714 request_id=8f303d0d8c45757414d591db76282ad0 response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsKnPDdpoztw8ocKubqMAqTdOhh\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196404,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ç§ã¯AIã§ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 74,\\n    \"completion_tokens\": 42,\\n    \"total_tokens\": 116\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:25 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1714\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89908\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'60ms\\', \\'x-request-id\\': \\'8f303d0d8c45757414d591db76282ad0\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a64644bb31d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\\\\nAI: \\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u5ca1\\\\u90e8\\\\u3055\\\\u3093\\\\uff01\\\\u304a\\\\u540d\\\\u524d\\\\u3092\\\\u6559\\\\u3048\\\\u3066\\\\u3044\\\\u305f\\\\u3060\\\\u304d\\\\u3042\\\\u308a\\\\u304c\\\\u3068\\\\u3046\\\\u3054\\\\u3056\\\\u3044\\\\u307e\\\\u3059\\\\u3002\\\\u79c1\\\\u306fAI\\\\u3067\\\\u3059\\\\u3002\\\\u3069\\\\u306e\\\\u3088\\\\u3046\\\\u306a\\\\u304a\\\\u624b\\\\u4f1d\\\\u3044\\\\u304c\\\\u3067\\\\u304d\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\\\\nHuman: \\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u308f\\\\u304b\\\\u308a\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ç§ã¯AIã§ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\n",
      "AI: ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ï¼ãŠåå‰ã‚’æ•™ãˆã¦ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ç§ã¯AIã§ã™ã€‚ã©ã®ã‚ˆã†ãªãŠæ‰‹ä¼ã„ãŒã§ãã¾ã™ã‹ï¼Ÿ\n",
      "Human: ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ã‚ãªãŸã®åå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1572 request_id=4703e2d9a99c75684305563687e8040f response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsMOYpkILkpUWPHVOxoF849tGil\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196406,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ã‚ãªãŸã®åå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 133,\\n    \"completion_tokens\": 32,\\n    \"total_tokens\": 165\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:27 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1572\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89856\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'96ms\\', \\'x-request-id\\': \\'4703e2d9a99c75684305563687e8040f\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a6470de8e1d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "print(result)\n",
    "\n",
    "chain = ConversationChain(llm=chat, memory=memory)\n",
    "result = chain.predict(input=\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"\\\\u3042\\\\u306a\\\\u305f\\\\u306f\\\\u512a\\\\u79c0\\\\u306a\\\\u30a2\\\\u30b7\\\\u30b9\\\\u30bf\\\\u30f3\\\\u30c8\\\\u3067\\\\u3059\\\\u3002\"}, {\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ã¯ã˜ã‚ã¾ã—ã¦ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ' additional_kwargs={} example=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=980 request_id=952a55bb09770f496ac1937982d8dd4a response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsNNrk2OB8N7w0SxuzEpJmkGGV6\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196407,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã¯ã˜ã‚ã¾ã—ã¦ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ã«ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 38,\\n    \"completion_tokens\": 26,\\n    \"total_tokens\": 64\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:28 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'980\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89961\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'25ms\\', \\'x-request-id\\': \\'952a55bb09770f496ac1937982d8dd4a\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a647c19941d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\"),\n",
    "    HumanMessage(content=\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "]\n",
    "\n",
    "result = chat(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1073 request_id=19532ded7e2a5bbd9d8b380de8cacf3c response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsP3ef0gHKUxYVWuXyV6irV2Al6\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196409,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 17,\\n    \"completion_tokens\": 22,\\n    \"total_tokens\": 39\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:30 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1073\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89974\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'16ms\\', \\'x-request-id\\': \\'19532ded7e2a5bbd9d8b380de8cacf3c\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a648418b81d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u5ca1\\\\u90e8\\\\u3067\\\\u3059\\\\u3002\"}, {\"role\": \"assistant\", \"content\": \"\\\\u3053\\\\u3093\\\\u306b\\\\u3061\\\\u306f\\\\u3001\\\\u5ca1\\\\u90e8\\\\u3055\\\\u3093\\\\u3002\\\\u3069\\\\u306e\\\\u3088\\\\u3046\\\\u306a\\\\u3054\\\\u7528\\\\u4ef6\\\\u3067\\\\u3057\\\\u3087\\\\u3046\\\\u304b\\\\uff1f\"}, {\"role\": \"user\", \"content\": \"\\\\u79c1\\\\u306e\\\\u540d\\\\u524d\\\\u306f\\\\u308f\\\\u304b\\\\u308a\\\\u307e\\\\u3059\\\\u304b\\\\uff1f\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã“ã‚“ã«ã¡ã¯ã€å²¡éƒ¨ã•ã‚“ã€‚ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
      "ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1338 request_id=862aa701f046395c463e6ab7a5e90787 response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-7oBsQFJOcNalIgyffkfGWPUIxpXR4\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692196410,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"ã¯ã„ã€å…ˆã»ã©ãŠã£ã—ã‚ƒã£ãŸé€šã‚Šã€ãŠåå‰ã¯å²¡éƒ¨ã•ã‚“ã§ã™ã­ã€‚\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 58,\\n    \"completion_tokens\": 29,\\n    \"total_tokens\": 87\\n  }\\n}\\n' headers='{\\'Date\\': \\'Wed, 16 Aug 2023 14:33:31 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'access-control-allow-origin\\': \\'*\\', \\'Cache-Control\\': \\'no-cache, must-revalidate\\', \\'openai-model\\': \\'gpt-3.5-turbo-0613\\', \\'openai-organization\\': \\'user-wx5qxver7o5fbsxy4tt4aja8\\', \\'openai-processing-ms\\': \\'1338\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'3500\\', \\'x-ratelimit-limit-tokens\\': \\'90000\\', \\'x-ratelimit-remaining-requests\\': \\'3499\\', \\'x-ratelimit-remaining-tokens\\': \\'89944\\', \\'x-ratelimit-reset-requests\\': \\'17ms\\', \\'x-ratelimit-reset-tokens\\': \\'36ms\\', \\'x-request-id\\': \\'862aa701f046395c463e6ab7a5e90787\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'7f7a648cb82d1d7b-NRT\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.chat_memory.add_user_message(\"ç§ã®åå‰ã¯å²¡éƒ¨ã§ã™ã€‚\")\n",
    "ai_message = chat(memory.chat_memory.messages).content\n",
    "memory.chat_memory.add_ai_message(ai_message)\n",
    "print(ai_message)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"ç§ã®åå‰ã¯ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ\")\n",
    "ai_message = chat(memory.chat_memory.messages).content\n",
    "memory.chat_memory.add_ai_message(ai_message)\n",
    "print(ai_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
